{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.5\n",
      "  Using cached https://files.pythonhosted.org/packages/04/79/a37d0b373757b4d283c674a64127bd8864d69f881c639b1ee5953e2d9301/tensorflow-1.5.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: tensorflow-tensorboard<1.6.0,>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.5)\n",
      "Collecting numpy>=1.12.1 (from tensorflow==1.5)\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/94/7049fed8373c52839c8cde619acaf2c9b83082b935e5aa8c0fa27a4a8bcc/numpy-1.15.1-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 19kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.5)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.5)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.5)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow==1.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5)\n",
      "Requirement already satisfied: bleach==1.5.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow==1.5)\n",
      "Installing collected packages: numpy, tensorflow\n",
      "Successfully installed numpy-1.15.1 tensorflow-1.5.0\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.chdir('/home/jovyan/work/data/conversation-structure-analysis/')\n",
    "#!pip install --upgrade --user tensorflow\n",
    "#!git clone https://github.com/tensorflow/tensorflow\n",
    "#!cd tensorflow\n",
    "#!git checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prerequisite_packages.ipynb\n",
    "%run preprocessing_functions.ipynb\n",
    "%run get_stopwords.ipynb\n",
    "%run import_hand_labelled_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "version = 1\n",
    "df, train_turns, test_turns, test_labels, lengths = get_samples(version, size_n = N, document_level = 'turn', agent = 'all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_turns = df['message'].apply(lambda x: str(x).lower().strip())\n",
    "train_turns = np.array(train_turns)\n",
    "\n",
    "\n",
    "def fasttext_text_vect(model, train_turns):\n",
    "    txt_path = \"\"\"./data/df_with_embedding.txt\"\"\"\n",
    "    with open(txt_path, 'w') as tmp:\n",
    "        for _ in train_turns:\n",
    "            tmp.write(\"%s\\n\" %_)    \n",
    "    return\n",
    "    #out_bytes = subprocess.check_output(\"\"\"../fastText-0.1.0/fasttext print-sentence-vectors \"\"\" + model + \"\"\" < \"\"\" + txt_path, shell = True)     \n",
    "    #return out_bytes.decode('utf-8')\n",
    "fasttext_text_vect(None, train_turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "with open('/home/jovyan/work/data/conversation-structure-analysis/data/pure_sentence_embedding.txt', 'r') as vec_file:\n",
    "    for l in vec_file.readlines():\n",
    "        embeddings.append([float(_) for _ in l.split(\" \")[-301:-1]])\n",
    "        \n",
    "#embeddings = np.array(embeddings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "arr = sparse.coo_matrix(embeddings, shape= np.array(embeddings).shape)\n",
    "df['newcol'] = arr.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./data/df_with_embedding.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>conv_uid</th>\n",
       "      <th>site_uid</th>\n",
       "      <th>message</th>\n",
       "      <th>agent</th>\n",
       "      <th>hand_label</th>\n",
       "      <th>newcol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ha-54621387</td>\n",
       "      <td>ha-4341</td>\n",
       "      <td>Bonjour si nous arrivons le dimanche matin ver...</td>\n",
       "      <td>visitor</td>\n",
       "      <td>greeting, ask_for_product_information</td>\n",
       "      <td>[0.065945, 0.010356, 0.0035353, 0.023578, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ha-54621387</td>\n",
       "      <td>ha-4341</td>\n",
       "      <td>Bonjour ! La réception de l’hôtel est ouverte ...</td>\n",
       "      <td>operator</td>\n",
       "      <td>greeting, send_product_information</td>\n",
       "      <td>[0.02754, 0.023644, 0.0081571, 0.0039379, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     conv_uid site_uid  \\\n",
       "0           0  ha-54621387  ha-4341   \n",
       "1           1  ha-54621387  ha-4341   \n",
       "\n",
       "                                             message     agent  \\\n",
       "0  Bonjour si nous arrivons le dimanche matin ver...   visitor   \n",
       "1  Bonjour ! La réception de l’hôtel est ouverte ...  operator   \n",
       "\n",
       "                              hand_label  \\\n",
       "0  greeting, ask_for_product_information   \n",
       "1     greeting, send_product_information   \n",
       "\n",
       "                                              newcol  \n",
       "0  [0.065945, 0.010356, 0.0035353, 0.023578, -0.0...  \n",
       "1  [0.02754, 0.023644, 0.0081571, 0.0039379, -0.0...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('./data/df_with_embedding.csv', sep = ',')\n",
    "df = df_all[0:5000]\n",
    "\n",
    "del df_all\n",
    "\n",
    "df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversations: 509\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "message_series = df['message']\n",
    "agent_series = df['agent']\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "count_v = count_a = 0\n",
    "\n",
    "\n",
    "grp_lst = df.groupby('conv_uid').groups.items() \n",
    "\n",
    "for key, items in grp_lst:\n",
    "    #print(key)\n",
    "    X_temp = []\n",
    "    Y_temp = []\n",
    "    items = list(items)\n",
    "    if df['agent'].iloc[items[0]]  == 'visitor':\n",
    "        \n",
    "        count_v +=1\n",
    "        \n",
    "    else:\n",
    "        count_a +=1\n",
    "    \n",
    "    \n",
    "    grp_agent = np.array([df['agent'].iloc[_] for _ in items])\n",
    "    #print ('grp_agent {}'.format(grp_agent))\n",
    "    visitor_turns_index = np.where(grp_agent == 'visitor')[0]\n",
    "    #print ('items:{}'.format(items))\n",
    "    \n",
    "    #print ('visitor index {}'.format(visitor_turns_index))\n",
    "    if len(visitor_turns_index)>0:\n",
    "        old_first = first = items[visitor_turns_index[0]]\n",
    "     #   print ('visitor first {}'.format(first))\n",
    "    \n",
    "        \n",
    "        while(first <= items[-1]):\n",
    "            \n",
    "            #print ('first {} and last {}'.format(first, items[-1]))\n",
    "        \n",
    "            #msg = df['message'].iloc[first]\n",
    "            vec = np.array([ast.literal_eval(df['newcol'].iloc[first])])\n",
    "            j = 1\n",
    "            while ((first+j) <= items[-1] and df['agent'].iloc[first+j] == df['agent'].iloc[old_first]):\n",
    "                #msg = msg + df['message'].iloc[first+j]\n",
    "                vec = vec + np.array([ast.literal_eval(df['newcol'].iloc[first+j])])\n",
    "                j = j+1\n",
    "            \n",
    "            \n",
    "            if df['agent'].iloc[old_first] == 'visitor' and old_first < items[-1]:\n",
    "            \n",
    "                X_temp.append((list(range(old_first, first+j)), vec*1.0/j))\n",
    "                \n",
    "            if df['agent'].iloc[old_first] == 'operator':\n",
    "                \n",
    "                Y_temp.append((list(range(old_first, first+j)), vec*1.0/j))\n",
    "            \n",
    "            old_first = first + j\n",
    "            first = first + j\n",
    "            \n",
    "            #print ('now first is {}'.format(first))\n",
    "    \n",
    "    if len(X_temp)>0 and len(Y_temp)>0:\n",
    "        X_train.append(X_temp)\n",
    "        Y_train.append(Y_temp)\n",
    "\n",
    "print('Number of conversations: {}'.format(len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of x_train 509\n",
      "longest turns: 29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([235., 152.,  64.,  29.,  12.,   9.,   6.,   0.,   1.,   1.]),\n",
       " array([ 1. ,  3.8,  6.6,  9.4, 12.2, 15. , 17.8, 20.6, 23.4, 26.2, 29. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train = [[_[1] for _ in x_seq] for x_seq in X_train]\n",
    "y_train = [[_[1] for _ in y_seq] for y_seq in Y_train]\n",
    "\n",
    "\n",
    "print ('length of x_train {}'.format(len(x_train)))\n",
    "#print ('length of Y_train {}'.format(len(Y_train)))\n",
    "\n",
    "lengths = [len(_) for _ in x_train]\n",
    "maxlen = max(lengths)\n",
    "print ('longest turns: {}'.format(maxlen))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "maxlen = 20\n",
    "\n",
    "def padding_turns(sequence, maxlen):\n",
    "    if len(sequence) < maxlen:\n",
    "        sequence = sequence + list(np.tile(np.zeros(shape = sequence[0].shape), ((maxlen - len(sequence)), 1,1))) ## [['PAD']]*(maxlen - len(sequence))\n",
    "    elif len(sequence) > maxlen:\n",
    "        sequence = sequence[0:maxlen]\n",
    "    else:\n",
    "        sequence = sequence\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "x_train = np.array([padding_turns(x_train[i], maxlen) for i in range(len(x_train))])\n",
    "\n",
    "y_train = np.array([padding_turns(y_train[i], maxlen) for i in range(len(y_train))])\n",
    "\n",
    "#print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=100,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "tf\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:(509, 20, 300)\n",
      "shape of y_train:(509, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "max_features = 300 \n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "batch_size = 32\n",
    "x_train = x_train.reshape((len(x_train), maxlen, 300))\n",
    "y_train = y_train.reshape((len(y_train), maxlen, 300))\n",
    "print ('shape of x_train:{}'.format(x_train.shape))\n",
    "print ('shape of y_train:{}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_11_input to have 2 dimensions, but got array with shape (509, 20, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-8c5a7b9e5ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m model.fit(x_train, y_train,\n\u001b[1;32m     12\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           epochs = 4)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1474\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1477\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_11_input to have 2 dimensions, but got array with shape (509, 20, 300)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_features, output_dim = 128, input_length = maxlen))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences= True, input_shape = (20, 300))))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(max_features, activation='relu'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'cosine_proximity', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size= batch_size,\n",
    "          epochs = 4)\n",
    "\n",
    "\n",
    "\n",
    "### next: meaning of embedding, how to yield hidden state of each cell of bidirectional lstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
